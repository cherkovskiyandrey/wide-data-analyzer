===========================================================================================================
===>>> Почему иногда на сигмоиде, когда мы застреваем веса меняют направление (+)
(см. в тетради гипотезу) + проверить:
СНАЧАЛА ПОДКЛЮЧИТЬ:
- сделать проект мавеновским (+)
- log4j чтобы можно было писать отладочную инфу (+)

после кажой эпохи для каждого веса строить график зависимости общей ошибки от каждого веса в его ближайшей окресности
так же выводить сообщение о смене знака дельты этого веса
Хочеться подтвердить теорию
Если она подтверждается то как тогда будет работать алгоритм Rprop ?
Ответ см. Caution: на стр. 95.

См. pendulum.xls
Комментарий:
Наблюдаем за любым Err(wi) - видим что до того момента когда мы обучились - график после каждой эпохи Err(wi - Delta <-> wi + Delta) при фиксации остальных весов
монотонный и не меняеться между эпохами, а потом начинается МАЯТНИК + нарастание ошибки происходит из-за погрешности вычислений в маятнике, т.к. нужно останавливатся в
этом моменте уже.
Вопрос - почему же маятник? (сделать функцию Err = f(wi, wj) и посмотреть если зависимость смены знака wi в области значений wj и если есть - дать сигнал) (-)
Вопрос - почему же сеть обучилась в
25-Dec-2017 16:18:05,001 MSK DEBUG - | Epoch: 20022; full relative error: 0.016706874999149142; current error: 0.002031510581508746; min error: 0.002031510581508746; TREND: decent;
Хотя если в маятнике посмотреть значений ошибки - они достигают меньшего результата: 3.581693	"0.001597" (?) - может локальный миниум


ОБНАРУЖЕНО:
Увеличил точность вывода цифр и теперь видно что даже в маятнике происходит постепенное уменьшение минимальной ошибки.
Т.е. она то последовательно в цикле n растёт, в цикле n-1 - обновляет минимальное значение, не правда не сильно существенно!


===========================================================================================================
===>>> Почему всё-таки мы застреваем в сигмоиде в 90% случаях, а в апроксимации гиперболического тангенса не застреваем вообще (+)
+ оказывается и в тангенсе тож застреваем в 50% случаев!!!!
- возможно не хватало возможностей нейронной сети, т.к. для такой сети:

                .inputsNeurons(2)
                .addHiddenLevel(20)
                .addHiddenLevel(20)
                .addHiddenLevel(20)
                .addHiddenLevel(20)
                .outputNeurons(3)

Получаем довольно быстро такие результаты:
15-Jan-2018 20:33:46,335 MSK DEBUG - Input: |  0.0000000000||  0.0000000000| => | -0.0021645008|| -0.0012672232|| -0.0000541552|
15-Jan-2018 20:33:46,335 MSK DEBUG - Input: |  0.0000000000||  1.0000000000| => |  0.0013704977||  0.0010928063||  0.9601600000|
15-Jan-2018 20:33:46,335 MSK DEBUG - Input: |  0.0000000000||  2.0000000000| => | -0.0003879812||  0.9601600000|| -0.0014055131|
15-Jan-2018 20:33:46,335 MSK DEBUG - Input: |  0.0000000000||  3.0000000000| => |  0.0180363532||  0.9601600000||  0.9601600000|
15-Jan-2018 20:33:46,335 MSK DEBUG - Input: |  0.0000000000||  4.0000000000| => |  0.9601600000|| -0.0009982358||  0.0157488933|
15-Jan-2018 20:33:46,335 MSK DEBUG - Input: |  0.0000000000||  5.0000000000| => |  0.9601600000||  0.0123391168||  0.9584562254|
15-Jan-2018 20:33:46,335 MSK DEBUG - Input: |  0.0000000000||  6.0000000000| => |  0.9601600000||  0.9601600000|| -0.0012663641|
15-Jan-2018 20:33:46,335 MSK DEBUG - Input: |  0.0000000000||  7.0000000000| => |  0.9601600000||  0.9601600000||  0.9601600000|
15-Jan-2018 20:33:46,335 MSK DEBUG - New minimum has been reached: 0.009962525386993621




===========================================================================================================
===>>> Проблема локального миниума:
https://www.researchgate.net/publication/220237893_Avoiding_the_Local_Minima_Problem_in_Backpropagation_Algorithm_with_Modified_Error_Function
Для архитектуры:
                .inputsNeurons(3)
                .addHiddenLevel(4)
                .outputNeurons(1)
И обучения:
                .setInputAndOutput(inputBuilder.setInputValues(Arrays.asList(0d, 0d, 0d)).build(), outputBuilder.setOutputValues(Arrays.asList(0d)).build())
                .setInputAndOutput(inputBuilder.setInputValues(Arrays.asList(0d, 0d, 1d)).build(), outputBuilder.setOutputValues(Arrays.asList(1d)).build())
                .setInputAndOutput(inputBuilder.setInputValues(Arrays.asList(0d, 1d, 0d)).build(), outputBuilder.setOutputValues(Arrays.asList(1d)).build())
                .setInputAndOutput(inputBuilder.setInputValues(Arrays.asList(0d, 1d, 1d)).build(), outputBuilder.setOutputValues(Arrays.asList(0d)).build())
                .setInputAndOutput(inputBuilder.setInputValues(Arrays.asList(1d, 0d, 0d)).build(), outputBuilder.setOutputValues(Arrays.asList(1d)).build())
                .setInputAndOutput(inputBuilder.setInputValues(Arrays.asList(1d, 0d, 1d)).build(), outputBuilder.setOutputValues(Arrays.asList(0d)).build())
                .setInputAndOutput(inputBuilder.setInputValues(Arrays.asList(1d, 1d, 0d)).build(), outputBuilder.setOutputValues(Arrays.asList(0d)).build())
                .setInputAndOutput(inputBuilder.setInputValues(Arrays.asList(1d, 1d, 1d)).build(), outputBuilder.setOutputValues(Arrays.asList(1d)).build())

иногда застреваем в локальном миниуме и видим картину:
19-Jan-2018 18:32:17,337 MSK DEBUG - | Epoch: 199999; full relative error: 0.11593218815980089; current error: 0.5006015428063131; min error: 0.5006015428063131; TREND: decent
|-------||-------||-------||-------||-------||-------||-------||-------|
|-------||-------||-------||-------||-------||-------||-------||-------|
|-------||-------||-------||-------||-------||-------||-------||-------|
|  0.107|| -0.111|| -0.111||-------||-------||-------||-------||-------|
|  0.336|| -2.116|| -2.116||-------||-------||-------||-------||-------|
| -0.232|| -1.761|| -1.761||-------||-------||-------||-------||-------|
| -0.376|| -1.761|| -1.761||-------||-------||-------||-------||-------|
|-------||-------||-------||-12.302||  5.036|| -1.182|| -3.853||-------|  <<<<<----------- большие значения

В статье даётся описание подобной ситуации, правда для сигмоиды, для неё всё ещё хуже чем для гиперболического тангенса Ангуита:
If weights connected to the hidden layer and the output layer are updated so in harmoniously that all the hidden neurons’
outputs are driven rapidly into the extreme areas before the output neurons start to approximate to the desired signals,
no weights connected to the hidden layer will be modified even though the actual outputs in the output layer are far from the desired out-puts.
Therefore, each hidden neuron generates a constant to the output layer.
The error signal cannot be propagated backwards, and the input signal cannot be passed forwards.
The hidden layer becomes meaningless. Therefore, the local minima problem occurs.





===========================================================================================================
===>>> Изменить подход остановки обучения (-)
Если нет проверочных сэмплов:
- например может гонять определённое время и обновлять минимальное значение + запоминать в нём топологию
	спустя это время выдавать топологию по минимальному значению

Если есть проверочные сэмплы:
- прогнать проверочные сэмплы и напрмер оценить % провальных, например если у нас используется граничный фильтр на выходе каждого нейрона - пропускаем
	через неё и если хоть один выход отличается то результат провальный

Нужно останавливаться, когда за заданное кол-во циклов у нас не происходит обнавление миниума ошибок или же оно совсем несущественное, меньше заданной дельты


===========================================================================================================
===>>> 5.6.4 Weight decay: Punishment of large weights (+)
Результаты однозначно хуже:

НО! так и должно быть! мы уменьшаем запоминаемость и увеличиваем свойсто обобщения сети


WITH WEIGHT DECAY:

22-Jan-2018 20:00:15,462 MSK DEBUG - Input: |  0.0000000000||  0.0000000000||  0.0000000000| => | -0.0000172205|
22-Jan-2018 20:00:15,462 MSK DEBUG - Input: |  0.0000000000||  0.0000000000||  1.0000000000| => |  0.7453497396|
22-Jan-2018 20:00:15,462 MSK DEBUG - Input: |  0.0000000000||  1.0000000000||  0.0000000000| => |  0.7356323836|
22-Jan-2018 20:00:15,462 MSK DEBUG - Input: |  0.0000000000||  1.0000000000||  1.0000000000| => |  0.0563257767|
22-Jan-2018 20:00:15,462 MSK DEBUG - Input: |  1.0000000000||  0.0000000000||  0.0000000000| => |  0.8745681218|
22-Jan-2018 20:00:15,462 MSK DEBUG - Input: |  1.0000000000||  0.0000000000||  1.0000000000| => |  0.1284565843|
22-Jan-2018 20:00:15,462 MSK DEBUG - Input: |  1.0000000000||  1.0000000000||  0.0000000000| => |  0.1323916862|
22-Jan-2018 20:00:15,462 MSK DEBUG - Input: |  1.0000000000||  1.0000000000||  1.0000000000| => |  0.8492134940|
22-Jan-2018 20:00:15,462 MSK DEBUG - New minimum has been reached: 0.34541104561368735
22-Jan-2018 20:00:15,462 MSK DEBUG - | Epoch: 78833; current error: 0.34541104561368735; min error: 0.34541104561368735; TREND: decent


Epoch: 99999;
22-Jan-2018 20:00:16,587 MSK DEBUG - NeuronNetworkImpl{NeuronNetworkDomain=NeuronNetworkDomain{inputAmount=3, outputAmount=1
, topology=
|-------||-------||-------||-------||-------||-------||-------||-------||-------||-------|
|-------||-------||-------||-------||-------||-------||-------||-------||-------||-------|
|-------||-------||-------||-------||-------||-------||-------||-------||-------||-------|
| -0.160||  0.125||  0.096||-------||-------||-------||-------||-------||-------||-------|
| -0.064||  0.055||  0.026||-------||-------||-------||-------||-------||-------||-------|
|  0.480||  1.440||  1.440||-------||-------||-------||-------||-------||-------||-------|
|  0.300||  1.735|| -1.968||-------||-------||-------||-------||-------||-------||-------|
| -0.160||  0.124||  0.096||-------||-------||-------||-------||-------||-------||-------|
| -0.413||  2.058|| -1.749||-------||-------||-------||-------||-------||-------||-------|
|-------||-------||-------||  2.948||  1.179||  0.393||  3.283||  2.936|| -3.390||-------|




WITHOUT WEIGTH DECAY:
22-Jan-2018 20:03:34,544 MSK DEBUG - | Epoch: 72969; current error: 0.0034781573787613894; min error: 0.0034781573787613894; TREND: decent
22-Jan-2018 20:03:34,544 MSK DEBUG - Input: |  0.0000000000||  0.0000000000||  0.0000000000| => | -0.0000173636|
22-Jan-2018 20:03:34,544 MSK DEBUG - Input: |  0.0000000000||  0.0000000000||  1.0000000000| => |  0.9575061062|
22-Jan-2018 20:03:34,544 MSK DEBUG - Input: |  0.0000000000||  1.0000000000||  0.0000000000| => |  0.9558374333|
22-Jan-2018 20:03:34,544 MSK DEBUG - Input: |  0.0000000000||  1.0000000000||  1.0000000000| => |  0.0034197983|
22-Jan-2018 20:03:34,544 MSK DEBUG - Input: |  1.0000000000||  0.0000000000||  0.0000000000| => |  0.9601600000|
22-Jan-2018 20:03:34,544 MSK DEBUG - Input: |  1.0000000000||  0.0000000000||  1.0000000000| => |  0.0023320318|
22-Jan-2018 20:03:34,544 MSK DEBUG - Input: |  1.0000000000||  1.0000000000||  0.0000000000| => |  0.0029410100|
22-Jan-2018 20:03:34,544 MSK DEBUG - Input: |  1.0000000000||  1.0000000000||  1.0000000000| => |  0.9601600000|
22-Jan-2018 20:03:34,544 MSK DEBUG - New minimum has been reached: 0.0034781488722984252


Epoch: 99999;
22-Jan-2018 20:03:35,845 MSK DEBUG - NeuronNetworkImpl{NeuronNetworkDomain=NeuronNetworkDomain{inputAmount=3, outputAmount=1
, topology=
|-------||-------||-------||-------||-------||-------||-------||-------||-------||-------|
|-------||-------||-------||-------||-------||-------||-------||-------||-------||-------|
|-------||-------||-------||-------||-------||-------||-------||-------||-------||-------|
| -0.074|| -1.501|| -1.651||-------||-------||-------||-------||-------||-------||-------|
| -0.310|| -1.816||  2.037||-------||-------||-------||-------||-------||-------||-------|
| -0.237||  0.188||  0.118||-------||-------||-------||-------||-------||-------||-------|
|  0.347|| -2.067||  1.824||-------||-------||-------||-------||-------||-------||-------|
|  0.413|| -1.345||  1.066||-------||-------||-------||-------||-------||-------||-------|
| -0.269||  0.218||  0.153||-------||-------||-------||-------||-------||-------||-------|
|-------||-------||-------|| -0.565|| -6.688||  3.829||  6.514||  0.520||  4.139||-------|

С другой стороны алгоритм действительно позволяет защищаться от больших значений
WITH WEIGTH DECAY:
Epoch: 2099999;
22-Jan-2018 20:13:43,586 MSK DEBUG - NeuronNetworkImpl{NeuronNetworkDomain=NeuronNetworkDomain{inputAmount=3, outputAmount=1
, topology=
|-------||-------||-------||-------||-------||-------||-------||-------|
|-------||-------||-------||-------||-------||-------||-------||-------|
|-------||-------||-------||-------||-------||-------||-------||-------|
|  0.304|| -1.967||  1.687||-------||-------||-------||-------||-------|
| -0.377|| -1.691||  2.022||-------||-------||-------||-------||-------|
|  0.419|| -0.338|| -0.361||-------||-------||-------||-------||-------|
|  0.073|| -0.290|| -0.426||-------||-------||-------||-------||-------|
|-------||-------||-------||  3.707|| -3.792|| -2.942|| -0.774||-------|


WITHOUT WEIGTH DECAY:
Epoch: 1799999;
22-Jan-2018 20:10:48,900 MSK DEBUG - NeuronNetworkImpl{NeuronNetworkDomain=NeuronNetworkDomain{inputAmount=3, outputAmount=1
, topology=
|-------||-------||-------||-------||-------||-------||-------||-------|
|-------||-------||-------||-------||-------||-------||-------||-------|
|-------||-------||-------||-------||-------||-------||-------||-------|
|  0.313||-11.931||-11.902||-------||-------||-------||-------||-------|
|  0.133||  0.891||  0.890||-------||-------||-------||-------||-------|
|  0.068||  1.858||  1.858||-------||-------||-------||-------||-------|
| -0.012||  0.942||  0.941||-------||-------||-------||-------||-------|
|-------||-------||-------|| 16.865||-21.052|| 19.253|| 19.268||-------|


============================================================================================================
===>>> Новый вид NN, способный различать капчи, по структуре схож с мозгом
https://www.vicarious.com/2017/10/26/common-sense-cortex-and-captcha/
https://github.com/vicariousinc/science_rcn
Так же код лежит на Я.ДИСК тут: Common Sense, Cortex, and CAPTCHA
Можно покурить, понять идею, реализовать на java например

















